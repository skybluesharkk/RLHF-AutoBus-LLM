import time
import numpy as np
import gymnasium as gym
from stable_baselines3 import PPO
from metadrive.envs import MetaDriveEnv
from stable_baselines3.common.monitor import Monitor


model_path = "ppo_metadrive_test_mycode"
model = PPO.load(model_path)
print(f"Loaded model from {model_path}")

env = MetaDriveEnv(dict(use_render=True,
                        map=4))
num_episodes = 10  
episode_rewards = []
episode_lengths = []

for ep in range(num_episodes):
    obs, _ = env.reset()
    total_reward = 0
    step_count = 0

    while True:
        #  ëª¨ë¸ì´ ì—°ì†ì ì¸ actionì„ ë°˜í™˜í•˜ë¯€ë¡œ í™˜ê²½ì— ë§ê²Œ ë³€í™˜ í•„ìš”
        action, _states = model.predict(obs, deterministic=True)

        if isinstance(env.action_space, gym.spaces.Discrete):
            action = int(action)  # ë‹¨ì¼ ì´ì‚° í–‰ë™ ë³€í™˜
        elif isinstance(env.action_space, gym.spaces.MultiDiscrete):
            action = action.astype(int)  # ì—¬ëŸ¬ ê°œì˜ ì´ì‚° í–‰ë™ ë³€í™˜

        obs, reward, done, _, _ = env.step(action)
        total_reward += reward
        step_count += 1

        # ë Œë”ë§ (ì‹œê°ì ìœ¼ë¡œ í™•ì¸)
        env.render(mode="human",map=4)
        time.sleep(0.05)  # ì†ë„ ì¡°ì ˆ

        if done:
            print(f"ğŸ¯ Episode {ep+1} finished - Total Reward: {total_reward}, Steps: {step_count}")
            episode_rewards.append(total_reward)
            episode_lengths.append(step_count)
            break

# í‰ê°€ ê²°ê³¼ ì¶œë ¥
avg_reward = np.mean(episode_rewards)
avg_steps = np.mean(episode_lengths)

print("\n**Evaluation Summary**")
print(f"Number of Episodes: {num_episodes}")
print(f"Average Reward: {avg_reward:.2f}")
print(f"Average Episode Length: {avg_steps:.2f}")

# í™˜ê²½ ì¢…ë£Œ
env.close()
